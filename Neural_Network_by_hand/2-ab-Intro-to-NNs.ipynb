{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"1X8QbVoXyVAW-pfcE9_W5tdA6BJ6TiBoS","timestamp":1666119390680},{"file_id":"1rDdcBxXhqI3AZjCFdDaJxChaIUoFqeGn","timestamp":1663765914763},{"file_id":"13wk-uH95GeGB0W5MU7RHRDf1PE8fzEag","timestamp":1603810050900}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"NfnKy8NB8J5e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666260249513,"user_tz":-120,"elapsed":60737,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}},"outputId":"083269bf-00c3-44f1-d333-ce23ec686137"},"source":["!wget https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n","!unzip -j 2-ab.zip\n","!wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py"],"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-20 10:03:07--  https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n","Resolving github.com (github.com)... 140.82.121.4\n","Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/data/2-ab.zip [following]\n","--2022-10-20 10:03:08--  https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/data/2-ab.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13423991 (13M) [application/zip]\n","Saving to: ‘2-ab.zip.1’\n","\n","2-ab.zip.1          100%[===================>]  12.80M  --.-KB/s    in 0.07s   \n","\n","2022-10-20 10:03:09 (182 MB/s) - ‘2-ab.zip.1’ saved [13423991/13423991]\n","\n","Archive:  2-ab.zip\n","replace ._2-ab? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace circles.mat? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n","  inflating: circles.mat             \n","  inflating: ._circles.mat           \n","  inflating: mnist.mat               \n","  inflating: ._mnist.mat             \n","  inflating: .DS_Store               \n","  inflating: ._.DS_Store             \n","--2022-10-20 10:04:08--  https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py\n","Resolving github.com (github.com)... 140.82.121.4\n","Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-ab/utils-data.py [following]\n","--2022-10-20 10:04:08--  https://raw.githubusercontent.com/rdfia/rdfia.github.io/master/code/2-ab/utils-data.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3950 (3.9K) [text/plain]\n","Saving to: ‘utils-data.py.1’\n","\n","utils-data.py.1     100%[===================>]   3.86K  --.-KB/s    in 0s      \n","\n","2022-10-20 10:04:08 (48.8 MB/s) - ‘utils-data.py.1’ saved [3950/3950]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"2vQ_LLdx8J5b","executionInfo":{"status":"ok","timestamp":1666260254355,"user_tz":-120,"elapsed":4848,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}},"colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"b60ff0e9-af87-4190-b008-39a498d1ff5b"},"source":["import math\n","import torch\n","from torch.autograd import Variable\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%run 'utils-data.py'\n","!pip install torchmetrics\n","from torchmetrics import Accuracy"],"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"48x_ha7f8J5i"},"source":["# Part 1 : Forward and backward passes \"by hands\""]},{"cell_type":"code","metadata":{"id":"GtizX1JV8J5n","executionInfo":{"status":"ok","timestamp":1666260254356,"user_tz":-120,"elapsed":15,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}}},"source":["def init_params(nx, nh, ny):\n","    \"\"\"\n","    nx, nh, ny: integers\n","    out params: dictionnary\n","    \"\"\"\n","    params = {}\n","    params[\"Wh\"] = 0.3 * torch.randn((nh,nx))\n","    params[\"Wy\"] = 0.3 * torch.randn((ny,nh))\n","    params[\"bh\"] = torch.zeros((1,nh))\n","    params[\"by\"] = torch.zeros((1,ny))\n","    return params\n","\n","def forward(params, X):\n","    \"\"\"\n","    params: dictionnary\n","    X: (n_batch, dimension)\n","    \"\"\"\n","    bsize = X.size(0)\n","    nh = params['Wh'].size(0)\n","    ny = params['Wy'].size(0)\n","    outputs = {}\n","\n","    outputs[\"X\"] = X\n","    outputs[\"htilde\"] = torch.mm(outputs[\"X\"],torch.transpose(params['Wh'],0,1)) \\\n","                        + params[\"bh\"].repeat(bsize,1)\n","    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n","    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"],torch.transpose(params['Wy'],0,1)) \\\n","                        + params[\"by\"].repeat(bsize,1)\n","    outputs[\"yhat\"] = torch.sum(torch.exp(outputs[\"ytilde\"]),1).resize(bsize,1) * torch.exp(outputs[\"ytilde\"])\n","\n","    return outputs['yhat'], outputs\n","  \n","def loss_accuracy(Yhat, Y):\n","\n","    L = - torch.mean(torch.sum(Y * torch.log(Yhat),1))\n","    _,indsYhat = torch.max(Yhat,1)\n","    _,indsY = torch.max(Y,1)\n","    acc = (torch.sum(torch.eq(indsYhat,indsY))/indsY.size(0))*100 \n","\n","    return L, acc\n","\n","def backward(params, outputs, Y):\n","    bsize = Y.shape[0]\n","    grads = {}\n","\n","    gradYtilt = outputs[\"yhat\"] - Y\n","    grads[\"Wy\"] = torch.mm(torch.transpose(gradYtilt,0,1),outputs[\"h\"])\n","    gradHtilt = torch.mm(gradYtilt,params[\"Wy\"]) * (torch.ones(outputs[\"h\"].size()) - outputs[\"h\"] * outputs[\"h\"])\n","    grads[\"Wh\"] = torch.mm(torch.transpose(gradHtilt,0,1),outputs[\"X\"])\n","    grads[\"by\"] = torch.sum(torch.transpose(gradYtilt,0,1),1)\n","    grads[\"bh\"] = torch.sum(torch.transpose(gradHtilt,0,1),1)\n","    return grads\n","\n","def sgd(params, grads, eta):\n","    \"\"\" Perform the stochastic gradient descent on the network \"\"\"\n","\n","    params[\"Wh\"] = params[\"Wh\"] - eta * grads[\"Wh\"]  \n","    params[\"Wy\"] = params[\"Wy\"] - eta * grads[\"Wy\"]\n","    params[\"bh\"] = params[\"bh\"] - eta * grads[\"bh\"]\n","    params[\"by\"] = params[\"by\"] - eta * grads[\"by\"]\n","\n","    return params"],"execution_count":96,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hifuW5UFA3DZ"},"source":["## Global learning procedure \"by hands\""]},{"cell_type":"code","metadata":{"id":"4RSw6bd0-qUe"},"source":["# init\n","N = data.Xtrain.shape[0]\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","\n","Nbatch = 10\n","N_epochs = 50\n","eta = 0.03\n","\n","params = init_params(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(N_epochs):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","        # forward pass (feeding the network of data by passing all the data across it)\n","        Yhat, outputs = forward(params,X)\n","\n","        # compute the loss and accuracy (between prediction: Yhat result of the forward pass and Y actual value)\n","        loss,acc = loss_accuracy(Yhat, Y)\n","\n","        # backward pass (calulate each partial derivative of the loss acccording each element of the network)\n","        grads = backward(params, outputs, Y)\n","        \n","        # perform gradient descent (update each bias and weight value of the network)\n","        params = sgd(params,grads,eta)\n","\n","    Yhat_train, _ = forward(params, data.Xtrain)\n","    Yhat_test, _ = forward(params, data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n","    Ygrid, _ = forward(params, data.Xgrid)  \n","\n","    print(iteration, acctrain, Ltrain.size(), acctest, Ltest.size())\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    data.plot_data_with_grid(Ygrid, title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OrHHH5PL8J54"},"source":["# Part 2 : Simplification of the backward pass with `torch.autograd`\n","\n"]},{"cell_type":"code","metadata":{"id":"hA4ycHlfBzCK"},"source":["def init_params(nx, nh, ny):\n","    \"\"\"\n","    nx, nh, ny: integers\n","    out params: dictionnary\n","    \"\"\"\n","    params = {}\n","    \n","    # activaye autograd on the network weights\n","    \n","    params[\"Wh\"] = torch.randn((nh,nx)) \n","    params[\"Wy\"] = torch.randn((ny,nh))\n","    params[\"bh\"] = torch.zeros(1,nh)\n","    params[\"by\"] = torch.zeros(1,ny)\n","    params[\"Wh\"].requires_grad = True\n","    params[\"Wy\"].requires_grad = True\n","    params[\"bh\"].requires_grad = True\n","    params[\"by\"].requires_grad = True\n","\n","    return params\n","\n","def sgd(params, eta):\n","    # update the network weights\n","    with torch.no_grad():\n","      print(params['Wy'].grad,params['Wh'].grad,params['by'].grad,params['bh'].grad)\n","      params['Wy'] -= eta * params['Wy'].grad\n","      params['Wh'] -= eta * params['Wh'].grad\n","      params['by'] -= eta * params['by'].grad\n","      params['bh'] -= eta * params['bh'].grad\n","      params['Wy'].grad.zero_()\n","      params['Wh'].grad.zero_()\n","      params['by'].grad.zero_()    \n","      params['bh'].grad.zero_()      \n","    return params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjgcmgQpDfOb"},"source":["## Global learning procedure with autograd"]},{"cell_type":"code","metadata":{"id":"8p5oR3EqDea-"},"source":["data = CirclesData()\n","data.plot_data()\n","\n","# init\n","N = data.Xtrain.shape[0]\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","\n","Nbatch = 5\n","N_epochs = 150\n","eta = 0.05\n","\n","params = init_params(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","\n","        Yhat, outs = forward(params, X)\n","        L,_ = loss_accuracy(Yhat, Y)\n","        L.backward()\n","        params = sgd(params,eta)\n","\n","    \n","    Yhat_train, _ = forward(params, data.Xtrain)\n","    Yhat_test, _ = forward(params, data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n","    Ygrid, _ = forward(params, data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title)\n","    # detach() is used to remove the predictions from the computational graph in autograd\n","    data.plot_data_with_grid(Ygrid.detach(), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","    #break \n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","plt.plot(curves[2], label=\"loss train\")\n","plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FV1iss68J6H"},"source":["# Part 3 : Simplification of the forward pass with `torch.nn`"]},{"cell_type":"markdown","metadata":{"id":"x6T5Uq7JEl47"},"source":["`init_params` and `forward` are replaced by the `init_model` function which defines the network architecture and the loss."]},{"cell_type":"code","metadata":{"id":"5-h4r-FH8J6I"},"source":["def init_model(nx, nh, ny):\n","  model = torch.nn.Sequential(\n","          torch.nn.Linear(nx, nh),\n","          torch.nn.Tanh(),\n","          torch.nn.Linear(nh, ny),\n","          torch.nn.Softmax(dim=1)\n","  )\n","  criterion = torch.nn.CrossEntropyLoss()\n","  return model, criterion\n","\n","def loss_accuracy(criterion,predictions,labels):\n","  L = criterion(predictions,labels)\n","  _,target = torch.max(labels,1)\n","  accuracy = Accuracy(top_k=1)\n","  print(predictions.size(),target.size())\n","  acc = accuracy(predictions, target.int())*100\n","  return L,acc\n","\n","def sgd(model, eta):\n","  with torch.no_grad():\n","    for param in model.parameters():\n","      param -= eta * param.grad\n","    model.zero_grad()\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOxBMmD4Gxtp"},"source":["## Global learning procedure with autograd and `torch.nn`"]},{"cell_type":"code","metadata":{"id":"4hMBmCNvHCLn","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1z9xYvIMNkYTAzDejam-iMBjXk_XKHVwR"},"executionInfo":{"status":"ok","timestamp":1666254601119,"user_tz":-120,"elapsed":26401,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}},"outputId":"6a64b037-f23d-4a11-bb50-28c3d4444005"},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 5\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.05\n","\n","model, criterion = init_model(nx, nh, ny)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(100):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","        \n","        # write the optimization algorithm on the batch (X,Y)\n","        # using the functions: loss_accuracy, sgd\n","        # the forward with the predict method from the model\n","        # and the backward function with autograd\n","\n","        Yhat = model(X)\n","        loss = criterion(Yhat, Y)\n","        loss.backward()\n","        model = sgd(model,eta)\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(criterion, Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(criterion, Yhat_test, data.Ytest)\n","    Ygrid = model(data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title) \n","    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","#plt.plot(curves[2], label=\"loss train\")\n","#plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"GoFSrQNsJCnz"},"source":["# Part 4 : Simplification of the SGD with `torch.optim`"]},{"cell_type":"code","metadata":{"id":"S8WtN9loJPqP","executionInfo":{"status":"ok","timestamp":1666260444143,"user_tz":-120,"elapsed":511,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}}},"source":["def init_model(nx, nh, ny, eta):\n","    model = torch.nn.Sequential(\n","            torch.nn.Linear(nx, nh),\n","            torch.nn.Tanh(),\n","            torch.nn.Linear(nh, ny),\n","            torch.nn.Softmax(dim=1)\n","    )\n","    loss = torch.nn.CrossEntropyLoss()\n","    optim = torch.optim.SGD(model.parameters(),lr=eta)\n","    return model, loss, optim\n","\n","def loss_accuracy(criterion,predictions,labels):\n","  L = criterion(predictions,labels)\n","  _,target = torch.max(labels,1)\n","  accuracy = Accuracy(top_k=1)\n","  acc = float(accuracy(predictions, target.int())*100)\n","  return L,acc"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eY-0rRzPJYDd"},"source":["The `sgd` function is replaced by calling the `optim.zero_grad()` before the backward and `optim.step()` after. "]},{"cell_type":"markdown","metadata":{"id":"q82hCupvJxvV"},"source":["## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"]},{"cell_type":"code","metadata":{"id":"V9h9nINKJ1LU","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17Y1GYcuZb_Gh0P6_4QDZrYDUzuslN0gJ"},"executionInfo":{"status":"ok","timestamp":1666260483665,"user_tz":-120,"elapsed":37814,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}},"outputId":"5738b923-e8f6-4cfd-d4ed-b2d402840c46"},"source":["# init\n","data = CirclesData()\n","data.plot_data()\n","N = data.Xtrain.shape[0]\n","Nbatch = 5\n","nx = data.Xtrain.shape[1]\n","nh = 10\n","ny = data.Ytrain.shape[1]\n","eta = 0.05\n","\n","model, criterion, optim = init_model(nx, nh, ny, eta)\n","\n","curves = [[],[], [], []]\n","\n","# epoch\n","for iteration in range(150):\n","\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","\n","        optim.zero_grad()\n","        \n","        Yhat = model(X)\n","        loss = criterion(Yhat, Y)\n","        loss.backward()\n","        optim.step()\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(criterion, Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(criterion, Yhat_test, data.Ytest)\n","    Ygrid = model(data.Xgrid)  \n","\n","    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n","    print(title) \n","    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","# plt.plot(curves[2], label=\"loss train\")\n","# plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":106,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Ts1s4JuOSaZ3"},"source":["# Part 5 : MNIST"]},{"cell_type":"markdown","metadata":{"id":"jly9C4FCSzLP"},"source":["Apply the code from previous part code to the MNIST dataset."]},{"cell_type":"code","metadata":{"id":"osrFoEr_Syi7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666256360760,"user_tz":-120,"elapsed":268188,"user":{"displayName":"hugo maitre","userId":"02543464689808418014"}},"outputId":"90f406f2-386d-45b5-8d12-2e04a577fa82"},"source":["# init\n","data = MNISTData()\n","N = data.Xtrain.shape[0]\n","Nbatch = 100\n","nx = data.Xtrain.shape[1]\n","nh = 100\n","ny = data.Ytrain.shape[1]\n","eta = 0.03\n","N_epochs = 150\n","\n","model, criterion, optim = init_model(nx, nh, ny, eta)\n","\n","for epoch in range(N_epochs):\n","    # permute\n","    perm = np.random.permutation(N)\n","    Xtrain = data.Xtrain[perm, :]\n","    Ytrain = data.Ytrain[perm, :]\n","\n","    # batches\n","    for j in range(N // Nbatch):\n","\n","        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n","        X = Xtrain[indsBatch, :]\n","        Y = Ytrain[indsBatch, :]\n","\n","        optim.zero_grad()\n","        \n","        Yhat = model(X)\n","        loss = criterion(Yhat, Y)\n","        loss.backward()\n","        optim.step()\n","\n","    Yhat_train = model(data.Xtrain)\n","    Yhat_test = model(data.Xtest)\n","    Ltrain, acctrain = loss_accuracy(criterion, Yhat_train, data.Ytrain)\n","    Ltest, acctest = loss_accuracy(criterion, Yhat_test, data.Ytest)\n","    print(\" Finishing Epoch n° {} , acctrain : {:.1f}%  , acctest : {:.1f}% \".format(str(epoch),acctrain,acctest))\n","\n","    curves[0].append(acctrain)\n","    curves[1].append(acctest)\n","    curves[2].append(Ltrain)\n","    curves[3].append(Ltest)\n","\n","fig = plt.figure()\n","plt.plot(curves[0], label=\"acc. train\")\n","plt.plot(curves[1], label=\"acc. test\")\n","# plt.plot(curves[2], label=\"loss train\")\n","# plt.plot(curves[3], label=\"loss test\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Finishing Epoch n° 0 , acctrain : 84.6%  , acctest : 85.4% \n"," Finishing Epoch n° 1 , acctrain : 89.9%  , acctest : 90.2% \n"," Finishing Epoch n° 2 , acctrain : 90.6%  , acctest : 90.5% \n"," Finishing Epoch n° 3 , acctrain : 91.3%  , acctest : 91.9% \n"," Finishing Epoch n° 4 , acctrain : 91.9%  , acctest : 92.1% \n"," Finishing Epoch n° 5 , acctrain : 92.1%  , acctest : 91.9% \n"," Finishing Epoch n° 6 , acctrain : 92.1%  , acctest : 92.0% \n"," Finishing Epoch n° 7 , acctrain : 92.3%  , acctest : 92.5% \n"," Finishing Epoch n° 8 , acctrain : 92.3%  , acctest : 92.6% \n"," Finishing Epoch n° 9 , acctrain : 92.1%  , acctest : 92.0% \n"," Finishing Epoch n° 10 , acctrain : 92.3%  , acctest : 92.2% \n"," Finishing Epoch n° 11 , acctrain : 92.5%  , acctest : 92.4% \n"," Finishing Epoch n° 12 , acctrain : 92.5%  , acctest : 92.0% \n"," Finishing Epoch n° 13 , acctrain : 92.8%  , acctest : 92.8% \n"," Finishing Epoch n° 14 , acctrain : 92.7%  , acctest : 92.8% \n"," Finishing Epoch n° 15 , acctrain : 93.1%  , acctest : 92.8% \n"," Finishing Epoch n° 16 , acctrain : 93.0%  , acctest : 93.0% \n"," Finishing Epoch n° 17 , acctrain : 93.4%  , acctest : 92.9% \n"," Finishing Epoch n° 18 , acctrain : 93.2%  , acctest : 92.9% \n"," Finishing Epoch n° 19 , acctrain : 93.3%  , acctest : 93.0% \n"," Finishing Epoch n° 20 , acctrain : 93.2%  , acctest : 93.0% \n"," Finishing Epoch n° 21 , acctrain : 93.3%  , acctest : 93.2% \n"," Finishing Epoch n° 22 , acctrain : 93.4%  , acctest : 93.4% \n"," Finishing Epoch n° 23 , acctrain : 93.6%  , acctest : 93.2% \n"," Finishing Epoch n° 24 , acctrain : 93.6%  , acctest : 93.6% \n"," Finishing Epoch n° 25 , acctrain : 93.6%  , acctest : 93.8% \n"," Finishing Epoch n° 26 , acctrain : 93.4%  , acctest : 93.5% \n"," Finishing Epoch n° 27 , acctrain : 93.8%  , acctest : 93.7% \n"," Finishing Epoch n° 28 , acctrain : 93.7%  , acctest : 93.7% \n"," Finishing Epoch n° 29 , acctrain : 94.0%  , acctest : 93.7% \n"," Finishing Epoch n° 30 , acctrain : 93.7%  , acctest : 93.5% \n"," Finishing Epoch n° 31 , acctrain : 93.5%  , acctest : 93.5% \n"," Finishing Epoch n° 32 , acctrain : 93.6%  , acctest : 93.5% \n"," Finishing Epoch n° 33 , acctrain : 93.6%  , acctest : 93.4% \n"," Finishing Epoch n° 34 , acctrain : 93.6%  , acctest : 93.5% \n"," Finishing Epoch n° 35 , acctrain : 93.9%  , acctest : 93.5% \n"," Finishing Epoch n° 36 , acctrain : 93.7%  , acctest : 93.6% \n"," Finishing Epoch n° 37 , acctrain : 93.7%  , acctest : 93.6% \n"," Finishing Epoch n° 38 , acctrain : 93.7%  , acctest : 93.7% \n"," Finishing Epoch n° 39 , acctrain : 94.0%  , acctest : 93.7% \n"," Finishing Epoch n° 40 , acctrain : 94.1%  , acctest : 93.8% \n"," Finishing Epoch n° 41 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 42 , acctrain : 94.1%  , acctest : 94.0% \n"," Finishing Epoch n° 43 , acctrain : 94.1%  , acctest : 93.7% \n"," Finishing Epoch n° 44 , acctrain : 94.1%  , acctest : 94.2% \n"," Finishing Epoch n° 45 , acctrain : 94.2%  , acctest : 93.9% \n"," Finishing Epoch n° 46 , acctrain : 94.1%  , acctest : 93.9% \n"," Finishing Epoch n° 47 , acctrain : 94.0%  , acctest : 93.8% \n"," Finishing Epoch n° 48 , acctrain : 94.2%  , acctest : 93.9% \n"," Finishing Epoch n° 49 , acctrain : 94.3%  , acctest : 94.1% \n"," Finishing Epoch n° 50 , acctrain : 94.2%  , acctest : 94.1% \n"," Finishing Epoch n° 51 , acctrain : 94.4%  , acctest : 94.2% \n"," Finishing Epoch n° 52 , acctrain : 94.2%  , acctest : 94.0% \n"," Finishing Epoch n° 53 , acctrain : 94.2%  , acctest : 94.1% \n"," Finishing Epoch n° 54 , acctrain : 94.3%  , acctest : 94.0% \n"," Finishing Epoch n° 55 , acctrain : 94.3%  , acctest : 94.1% \n"," Finishing Epoch n° 56 , acctrain : 94.4%  , acctest : 94.2% \n"," Finishing Epoch n° 57 , acctrain : 94.4%  , acctest : 94.2% \n"," Finishing Epoch n° 58 , acctrain : 94.3%  , acctest : 94.1% \n"," Finishing Epoch n° 59 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 60 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 61 , acctrain : 94.5%  , acctest : 93.8% \n"," Finishing Epoch n° 62 , acctrain : 94.6%  , acctest : 94.1% \n"," Finishing Epoch n° 63 , acctrain : 94.3%  , acctest : 94.0% \n"," Finishing Epoch n° 64 , acctrain : 94.1%  , acctest : 93.9% \n"," Finishing Epoch n° 65 , acctrain : 94.2%  , acctest : 94.2% \n"," Finishing Epoch n° 66 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 67 , acctrain : 94.6%  , acctest : 94.1% \n"," Finishing Epoch n° 68 , acctrain : 94.5%  , acctest : 94.3% \n"," Finishing Epoch n° 69 , acctrain : 94.5%  , acctest : 94.3% \n"," Finishing Epoch n° 70 , acctrain : 94.5%  , acctest : 93.9% \n"," Finishing Epoch n° 71 , acctrain : 94.2%  , acctest : 93.9% \n"," Finishing Epoch n° 72 , acctrain : 94.6%  , acctest : 94.2% \n"," Finishing Epoch n° 73 , acctrain : 94.6%  , acctest : 94.4% \n"," Finishing Epoch n° 74 , acctrain : 94.7%  , acctest : 94.3% \n"," Finishing Epoch n° 75 , acctrain : 94.6%  , acctest : 94.4% \n"," Finishing Epoch n° 76 , acctrain : 94.4%  , acctest : 94.3% \n"," Finishing Epoch n° 77 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 78 , acctrain : 94.6%  , acctest : 94.5% \n"," Finishing Epoch n° 79 , acctrain : 94.5%  , acctest : 94.3% \n"," Finishing Epoch n° 80 , acctrain : 94.6%  , acctest : 94.2% \n"," Finishing Epoch n° 81 , acctrain : 94.3%  , acctest : 94.3% \n"," Finishing Epoch n° 82 , acctrain : 94.7%  , acctest : 94.3% \n"," Finishing Epoch n° 83 , acctrain : 94.3%  , acctest : 94.2% \n"," Finishing Epoch n° 84 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 85 , acctrain : 94.6%  , acctest : 94.3% \n"," Finishing Epoch n° 86 , acctrain : 94.7%  , acctest : 94.5% \n"," Finishing Epoch n° 87 , acctrain : 94.7%  , acctest : 94.5% \n"," Finishing Epoch n° 88 , acctrain : 94.9%  , acctest : 94.6% \n"," Finishing Epoch n° 89 , acctrain : 94.7%  , acctest : 94.4% \n"," Finishing Epoch n° 90 , acctrain : 94.8%  , acctest : 94.7% \n"," Finishing Epoch n° 91 , acctrain : 94.8%  , acctest : 94.5% \n"," Finishing Epoch n° 92 , acctrain : 94.7%  , acctest : 94.3% \n"," Finishing Epoch n° 93 , acctrain : 94.8%  , acctest : 94.5% \n"," Finishing Epoch n° 94 , acctrain : 94.6%  , acctest : 94.5% \n"," Finishing Epoch n° 95 , acctrain : 94.4%  , acctest : 94.0% \n"," Finishing Epoch n° 96 , acctrain : 94.8%  , acctest : 94.5% \n"," Finishing Epoch n° 97 , acctrain : 95.0%  , acctest : 94.6% \n"," Finishing Epoch n° 98 , acctrain : 94.7%  , acctest : 94.6% \n"," Finishing Epoch n° 99 , acctrain : 94.4%  , acctest : 94.1% \n"," Finishing Epoch n° 100 , acctrain : 94.8%  , acctest : 94.3% \n"," Finishing Epoch n° 101 , acctrain : 94.6%  , acctest : 94.2% \n"," Finishing Epoch n° 102 , acctrain : 94.7%  , acctest : 94.4% \n"," Finishing Epoch n° 103 , acctrain : 95.0%  , acctest : 94.7% \n"," Finishing Epoch n° 104 , acctrain : 95.1%  , acctest : 94.6% \n"," Finishing Epoch n° 105 , acctrain : 95.0%  , acctest : 94.6% \n"," Finishing Epoch n° 106 , acctrain : 95.2%  , acctest : 94.8% \n"," Finishing Epoch n° 107 , acctrain : 95.1%  , acctest : 94.7% \n"," Finishing Epoch n° 108 , acctrain : 95.1%  , acctest : 94.6% \n"," Finishing Epoch n° 109 , acctrain : 95.1%  , acctest : 94.5% \n"," Finishing Epoch n° 110 , acctrain : 95.1%  , acctest : 94.7% \n"," Finishing Epoch n° 111 , acctrain : 95.0%  , acctest : 94.6% \n"," Finishing Epoch n° 112 , acctrain : 94.9%  , acctest : 94.1% \n"," Finishing Epoch n° 113 , acctrain : 94.9%  , acctest : 94.8% \n"," Finishing Epoch n° 114 , acctrain : 94.7%  , acctest : 94.3% \n"," Finishing Epoch n° 115 , acctrain : 94.7%  , acctest : 94.3% \n"," Finishing Epoch n° 116 , acctrain : 94.9%  , acctest : 94.3% \n"," Finishing Epoch n° 117 , acctrain : 94.9%  , acctest : 94.2% \n"," Finishing Epoch n° 118 , acctrain : 95.3%  , acctest : 94.8% \n"," Finishing Epoch n° 119 , acctrain : 95.3%  , acctest : 94.7% \n"," Finishing Epoch n° 120 , acctrain : 95.1%  , acctest : 94.6% \n"," Finishing Epoch n° 121 , acctrain : 95.2%  , acctest : 94.8% \n"," Finishing Epoch n° 122 , acctrain : 95.2%  , acctest : 94.8% \n"," Finishing Epoch n° 123 , acctrain : 95.2%  , acctest : 94.7% \n"," Finishing Epoch n° 124 , acctrain : 95.2%  , acctest : 94.9% \n"," Finishing Epoch n° 125 , acctrain : 95.3%  , acctest : 95.0% \n"," Finishing Epoch n° 126 , acctrain : 95.3%  , acctest : 94.7% \n"," Finishing Epoch n° 127 , acctrain : 95.2%  , acctest : 94.6% \n"," Finishing Epoch n° 128 , acctrain : 94.9%  , acctest : 94.5% \n"," Finishing Epoch n° 129 , acctrain : 95.1%  , acctest : 94.9% \n"," Finishing Epoch n° 130 , acctrain : 95.1%  , acctest : 94.8% \n"," Finishing Epoch n° 131 , acctrain : 95.3%  , acctest : 95.0% \n"," Finishing Epoch n° 132 , acctrain : 95.1%  , acctest : 94.5% \n"," Finishing Epoch n° 133 , acctrain : 95.1%  , acctest : 94.8% \n"," Finishing Epoch n° 134 , acctrain : 95.1%  , acctest : 94.5% \n"," Finishing Epoch n° 135 , acctrain : 95.2%  , acctest : 94.8% \n"," Finishing Epoch n° 136 , acctrain : 95.4%  , acctest : 94.8% \n"," Finishing Epoch n° 137 , acctrain : 95.1%  , acctest : 94.8% \n"," Finishing Epoch n° 138 , acctrain : 95.0%  , acctest : 94.8% \n"," Finishing Epoch n° 139 , acctrain : 94.9%  , acctest : 94.6% \n"," Finishing Epoch n° 140 , acctrain : 95.0%  , acctest : 94.7% \n"," Finishing Epoch n° 141 , acctrain : 95.2%  , acctest : 94.8% \n"," Finishing Epoch n° 142 , acctrain : 95.1%  , acctest : 94.6% \n"," Finishing Epoch n° 143 , acctrain : 94.9%  , acctest : 94.4% \n"," Finishing Epoch n° 144 , acctrain : 94.9%  , acctest : 94.2% \n"," Finishing Epoch n° 145 , acctrain : 95.2%  , acctest : 94.5% \n"," Finishing Epoch n° 146 , acctrain : 95.1%  , acctest : 94.7% \n"," Finishing Epoch n° 147 , acctrain : 95.3%  , acctest : 94.5% \n"," Finishing Epoch n° 148 , acctrain : 95.2%  , acctest : 95.0% \n"," Finishing Epoch n° 149 , acctrain : 95.3%  , acctest : 94.7% \n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"YRoiGbhvmSLO"},"source":["# Part 6: Bonus: SVM\n","\n","\n","Train a SVM model on the Circles dataset.\n","\n","Ideas : \n","- First try a linear SVM (sklearn.svm.LinearSVC dans scikit-learn). Does it work well ? Why ?\n","- Then try more complex kernels (sklearn.svm.SVC). Which one is the best ? why ?\n","- Does the parameter C of regularization have an impact? Why ?"]},{"cell_type":"code","metadata":{"id":"VWeW8siymR3g"},"source":["# data\n","data = CirclesData()\n","Xtrain = data.Xtrain.numpy()\n","Ytrain = data.Ytrain[:, 0].numpy()\n","\n","Xgrid = data.Xgrid.numpy()\n","\n","Xtest = data.Xtest.numpy()\n","Ytest = data.Ytest[:, 0].numpy()\n","\n","def plot_svm_predictions(data, predictions):\n","      plt.figure(2)\n","      plt.clf()\n","      plt.imshow(np.reshape(predictions, (40,40)))\n","      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n","      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n","      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n","      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n","      plt.xlim(0,39)\n","      plt.ylim(0,39)\n","      plt.clim(0.3,0.7)\n","      plt.draw()\n","      plt.pause(1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1xcE6zbmXU1"},"source":["import sklearn.svm\n","\n","############################\n","### Your code here   #######\n","### Train the SVM    #######\n","## See https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n","## and https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","############################\n","\n","svm = None\n","\n","###########################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgLl7B_3mbOs"},"source":["## Print results\n","\n","Ytest_pred = svm.predict(Xtest)\n","accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n","print(f\"Accuracy : {100 * accuracy:.2f}\")\n","Ygrid_pred = svm.predict(Xgrid)\n","plot_svm_predictions(data, Ygrid_pred)"],"execution_count":null,"outputs":[]}]}